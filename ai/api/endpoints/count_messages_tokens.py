"""
API endpoint for count_messages_tokens
Generated by Advanced Feature Stub Generator
"""
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field, validator
from typing import Optional, Dict, Any, List
from datetime import datetime

# Optional dependency – if not installed the service will gracefully degrade
try:
    import tiktoken  # type: ignore
except ImportError:  # pragma: no cover
    tiktoken = None  # Fallback to naive tokenizer if package missing

router = APIRouter()


class Message(BaseModel):
    """Represents a single chat/completion message."""
    role: str = Field(..., description="The role of the message sender")
    content: str = Field(..., description="The textual content of the message")

    @validator("role")
    def role_must_not_be_empty(cls, v: str) -> str:  # noqa: N805
        if not v.strip():
            raise ValueError("role must not be empty")
        return v

    @validator("content")
    def content_must_not_be_empty(cls, v: str) -> str:  # noqa: N805
        if not v.strip():
            raise ValueError("content must not be empty")
        return v


class count_messages_tokensRequest(BaseModel):
    """Request model for count_messages_tokens."""
    model: str = Field(
        default="gpt-3.5-turbo-0613",
        description="The name of the model the tokeniser should mimic"
    )
    messages: List[Message] = Field(
        ..., description="List of messages whose tokens should be counted"
    )
    data: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Optional extra data passed through unchanged"
    )


class count_messages_tokensResponse(BaseModel):
    """Response model for count_messages_tokens."""
    success: bool
    message: str
    data: Optional[Dict[str, Any]] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)


def _num_tokens_from_messages(messages: List[Message], model: str) -> int:
    """
    Calculate the number of tokens a list of messages would consume for a given model.

    If `tiktoken` is available the model-specific encoding is used. Otherwise a simple
    whitespace split fallback is applied (this will under-count compared to the real
    encoder but guarantees the endpoint remains available even without the dependency).
    """
    # Use OpenAI recommended method if tiktoken is installed
    if tiktoken:
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            # Fallback to cl100k_base if model not recognised
            encoding = tiktoken.get_encoding("cl100k_base")
        # Per-message and per-name tokens vary by model
        if model.startswith("gpt-4"):
            tokens_per_message = 3
            tokens_per_name = 1
        else:  # Includes gpt-3.5-turbo and others
            tokens_per_message = 4
            tokens_per_name = -1
        total_tokens = 0
        for message in messages:
            total_tokens += tokens_per_message
            total_tokens += len(encoding.encode(message.content))
            total_tokens += len(encoding.encode(message.role))
            if message.role:
                total_tokens += tokens_per_name
        # Every reply is primed with <|assistant|>
        total_tokens += 3
        return max(total_tokens, 0)
    # Naive fallback: count whitespace-separated tokens
    return sum(len((message.content or "").split()) + len((message.role or "").split()) for message in messages)


@router.post("/api/v1/count-messages-tokens", response_model=count_messages_tokensResponse)
async def count_messages_tokens_endpoint(
    request: count_messages_tokensRequest
) -> count_messages_tokensResponse:
    """
    Count the approximate number of tokens contained in a sequence of messages.

    The endpoint supports both accurate counting (when the `tiktoken` package is
    present) and a graceful degradation to a naive counter if the dependency is
    unavailable.
    """
    try:
        # Basic validation – ensure we have at least one message
        if not request.messages:
            raise HTTPException(status_code=422, detail="`messages` list must not be empty")

        token_count = _num_tokens_from_messages(request.messages, request.model)

        result_data: Dict[str, Any] = {
            "model": request.model,
            "messages_count": len(request.messages),
            "tokens": token_count,
            "tokenizer": "tiktoken" if tiktoken else "naive",
        }

        # Echo back any additional data provided by the caller
        if request.data is not None:
            result_data["request_data"] = request.data

        return count_messages_tokensResponse(
            success=True,
            message="Token count calculated successfully",
            data=result_data
        )

    except HTTPException:
        # Re-raise HTTPExceptions untouched so FastAPI can handle status codes
        raise
    except Exception as e:  # pragma: no cover
        # Catch-all for unexpected errors
        raise HTTPException(
            status_code=500,
            detail=f"count_messages_tokens execution failed: {str(e)}"
        ) from e


# Export router for main application
__all__ = ["router", "count_messages_tokensRequest", "count_messages_tokensResponse"]