#!/usr/bin/env python3
"""
Parallel Autonomous Implementation Orchestrator
Uses MotherAgent to spawn multiple autonomous workers in parallel
"""
import asyncio
import os
import sys
import time
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import concurrent.futures
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Load environment
def load_env():
    """Load environment variables from .env file"""
    env_file = Path(__file__).parent.parent / ".env"
    if env_file.exists():
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key] = value.strip('"\'')

load_env()

@dataclass
class ParallelJob:
    """Represents a parallel implementation job"""
    id: str
    file_path: str
    description: str
    status: str = "pending"  # pending, running, success, failed
    model_used: Optional[str] = None
    duration: Optional[float] = None
    cost: Optional[float] = None
    error_message: Optional[str] = None
    start_time: Optional[float] = None
    end_time: Optional[float] = None

class ParallelAutonomousOrchestrator:
    """Orchestrates parallel autonomous implementation through MotherAgent"""
    
    def __init__(self, max_workers: int = 10, budget_limit: float = 5.0):
        self.max_workers = max_workers
        self.budget_limit = budget_limit
        self.total_cost = 0.0
        self.jobs: List[ParallelJob] = []
        self.setup_openai()
    
    def setup_openai(self):
        """Setup OpenAI client"""
        try:
            from openai import OpenAI
            
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                raise ValueError("OPENAI_API_KEY not found in environment")
            
            self.client = OpenAI(api_key=api_key)
            print("âœ… OpenAI client configured for parallel orchestration")
        except ImportError:
            print("âŒ OpenAI package not found. Please install: pip install openai")
            sys.exit(1)
    
    def get_implementation_prompt(self, file_path: str, current_code: str, feature_description: str) -> str:
        """Create implementation prompt for MotherAgent spawned worker"""
        return f"""MotherAgent Task: Implement {feature_description}

File: {file_path}

Current code:
{current_code}

Instructions:
- Replace ALL TODO comments with working code
- Keep existing imports, classes, and function signatures
- Add proper error handling and validation
- Make it production-ready and functional
- For CLI: add proper Click decorators and help
- For API: return proper JSON responses

IMPORTANT: Return ONLY the complete Python code, no explanations or markdown."""

    async def implement_feature_async(self, job: ParallelJob) -> ParallelJob:
        """Async implementation of a single feature"""
        job.status = "running"
        job.start_time = time.time()
        
        try:
            # Read current stub
            full_path = Path(job.file_path)
            if not full_path.exists():
                job.status = "failed"
                job.error_message = f"File not found: {job.file_path}"
                return job
            
            with open(full_path, 'r') as f:
                current_code = f.read()
            
            # Create prompt
            prompt = self.get_implementation_prompt(job.file_path, current_code, job.description)
            
            # Production configuration: GPT-5 (high reasoning) â†’ GPT-4o-mini fallback only
            # GPT-5: high reasoning for complex autonomous coding tasks
            # GPT-4o-mini: fast fallback for simpler tasks
            models_to_try = ["gpt-5", "gpt-4o-mini"]  # GPT-5 first for quality
            
            implementation = None
            successful_model = None
            
            for model in models_to_try:
                try:
                    reasoning = "HIGH" if "planning" in job.description.lower() or "strategy" in job.description.lower() else "MEDIUM"
                    print(f"ðŸ¤– Job {job.id}: Using {model} (reasoning_effort={reasoning.lower()}) - {job.description[:50]}...")
                    
                    # Prepare API parameters
                    api_params = {
                        "model": model,
                        "messages": [
                            {"role": "system", "content": "You are a MotherAgent spawned autonomous worker. Return only working Python code, no explanations."},
                            {"role": "user", "content": prompt}
                        ]
                    }
                    
                    # Smart MotherAgent model parameters with pattern-based reasoning
                    if model.startswith("gpt-5"):
                        # Intelligent reasoning effort based on task complexity
                        task_description = job.description.lower()
                        complex_patterns = ["debug", "fix", "optimize", "architecture", "workflow", "pipeline", "multi-step", "agent"]
                        
                        if any(pattern in task_description for pattern in complex_patterns):
                            reasoning_effort = "high"
                            verbosity = "medium"
                        else:
                            reasoning_effort = "medium" 
                            verbosity = "low"
                        
                        api_params["max_completion_tokens"] = 2500
                        api_params["reasoning_effort"] = reasoning_effort
                        api_params["verbosity"] = verbosity
                        print(f"ðŸ§  GPT-5 reasoning: {reasoning_effort}, verbosity: {verbosity}")
                    else:
                        api_params["max_tokens"] = 2500
                        api_params["temperature"] = 0.1
                        api_params["timeout"] = 30
                    
                    # Make API call using thread pool for true concurrency
                    loop = asyncio.get_event_loop()
                    with ThreadPoolExecutor(max_workers=1) as executor:
                        response = await loop.run_in_executor(
                            executor, 
                            lambda: self.client.chat.completions.create(**api_params)
                        )
                    content = response.choices[0].message.content
                    
                    # Clean up markdown formatting
                    if content.startswith('```python\n'):
                        content = content[10:]
                    if content.endswith('\n```'):
                        content = content[:-4]
                    if content.endswith('```'):
                        content = content[:-3]
                    
                    # Estimate cost
                    if hasattr(response, 'usage') and response.usage:
                        prompt_tokens = response.usage.prompt_tokens
                        completion_tokens = response.usage.completion_tokens
                        cost = (prompt_tokens / 1000 * 0.01) + (completion_tokens / 1000 * 0.03)
                    else:
                        cost = (len(prompt) + len(content)) / 1000 * 0.002
                    
                    job.cost = cost
                    self.total_cost += cost
                    
                    if len(content) > 100 and "TODO" not in content:
                        implementation = content
                        successful_model = model
                        print(f"âœ… Job {job.id}: Success with {model}")
                        break
                    else:
                        print(f"âš ï¸ Job {job.id}: {model} returned incomplete implementation")
                        
                except Exception as e:
                    print(f"âŒ Job {job.id}: {model} failed: {str(e)}")
                    continue
            
            if not implementation:
                job.status = "failed"
                job.error_message = "All models failed to produce valid implementation"
                return job
            
            # Backup original and write new implementation
            backup_path = full_path.with_suffix(full_path.suffix + '.backup')
            with open(backup_path, 'w') as f:
                f.write(current_code)
            
            with open(full_path, 'w') as f:
                f.write(implementation)
            
            job.status = "success"
            job.model_used = successful_model
            job.end_time = time.time()
            job.duration = job.end_time - job.start_time
            
            print(f"âœ… Job {job.id}: Successfully implemented {job.file_path} using {successful_model} (${job.cost:.3f}, {job.duration:.1f}s)")
            return job
            
        except Exception as e:
            job.status = "failed"
            job.error_message = str(e)
            job.end_time = time.time()
            if job.start_time:
                job.duration = job.end_time - job.start_time
            print(f"âŒ Job {job.id}: Failed with error: {str(e)}")
            return job
    
    async def run_parallel_batch(self, features: List[Dict]) -> Dict:
        """Run batch of features in parallel using MotherAgent orchestration"""
        print(f"\nðŸš€ MotherAgent: Spawning {len(features)} autonomous workers in PARALLEL")
        print(f"ðŸ’° Budget limit: ${self.budget_limit}")
        print(f"ðŸ‘¥ Max concurrent workers: {self.max_workers}")
        print(f"ðŸ¤– Production models: GPT-5 (high reasoning primary) â†’ GPT-4o-mini (fast fallback)")
        print(f"âš¡ TRUE PARALLEL EXECUTION - All {len(features)} workers will run simultaneously!")
        print("=" * 80)
        
        # Create jobs
        self.jobs = []
        for i, feature in enumerate(features):
            job = ParallelJob(
                id=f"worker-{i+1:02d}",
                file_path=feature["file_path"],
                description=feature["description"]
            )
            self.jobs.append(job)
        
        start_time = time.time()
        
        # Run jobs in parallel with progress monitoring
        semaphore = asyncio.Semaphore(self.max_workers)
        
        async def run_with_semaphore(job):
            async with semaphore:
                return await self.implement_feature_async(job)
        
        # Start progress monitoring
        progress_task = asyncio.create_task(self.monitor_progress())
        
        # Execute all jobs in TRUE PARALLEL using asyncio.gather
        print(f"ðŸš€ Launching {len(self.jobs)} workers in PARALLEL...")
        tasks = [run_with_semaphore(job) for job in self.jobs]
        
        # This is the key: asyncio.gather runs ALL tasks concurrently
        completed_jobs = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Stop progress monitoring
        progress_task.cancel()
        
        end_time = time.time()
        duration = end_time - start_time
        
        # Count results
        successful = sum(1 for job in self.jobs if job.status == "success")
        failed = sum(1 for job in self.jobs if job.status == "failed")
        
        # Generate final report
        report = {
            "orchestration_summary": {
                "total_features": len(features),
                "successful": successful,
                "failed": failed,
                "success_rate": successful / len(features) if features else 0,
                "total_cost": self.total_cost,
                "duration": duration,
                "timestamp": datetime.now().isoformat(),
                "parallel_workers": self.max_workers,
                "orchestrator": "MotherAgent"
            },
            "job_details": [asdict(job) for job in self.jobs]
        }
        
        print(f"\nðŸŽ¯ PARALLEL ORCHESTRATION COMPLETE!")
        print(f"âœ… Successful: {successful}")
        print(f"âŒ Failed: {failed}")
        print(f"ðŸ’° Total cost: ${self.total_cost:.2f}")
        print(f"âš¡ Duration: {duration:.1f} seconds (parallel execution!)")
        print(f"ðŸ¤– MotherAgent orchestrated {self.max_workers} concurrent workers")
        
        # Save report
        report_path = Path("logs/parallel_implementation_report.json")
        report_path.parent.mkdir(exist_ok=True)
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"ðŸ“Š Report saved: {report_path}")
        
        return report
    
    async def monitor_progress(self):
        """Real-time progress monitoring"""
        try:
            while True:
                await asyncio.sleep(5)  # Update every 5 seconds
                
                pending = sum(1 for job in self.jobs if job.status == "pending")
                running = sum(1 for job in self.jobs if job.status == "running")
                success = sum(1 for job in self.jobs if job.status == "success")
                failed = sum(1 for job in self.jobs if job.status == "failed")
                
                print(f"ðŸ“Š Progress: â³ {pending} pending | ðŸ”„ {running} running | âœ… {success} success | âŒ {failed} failed | ðŸ’° ${self.total_cost:.2f}")
                
        except asyncio.CancelledError:
            pass

async def main():
    """Main execution function"""
    print("ðŸš€ Fresh AI Parallel Autonomous Implementation Orchestrator")
    print("ðŸ¤– MotherAgent â†’ 10 Parallel Autonomous Workers â†’ Real-time Implementation")
    print("=" * 80)
    
    # Select 20 real TODO stub features for parallel implementation with GPT-4o-mini
    test_features = [
        {"file_path": "ai/cli/commands/register_context.py", "description": "Context registration and management"},
        {"file_path": "ai/cli/commands/selfdocumentingloopservice.py", "description": "Self-documenting loop service with validation"},
        {"file_path": "ai/cli/commands/connect.py", "description": "Connection management and networking utilities"},
        {"file_path": "ai/cli/commands/codebasemonitor.py", "description": "Codebase monitoring and change detection"},
        {"file_path": "ai/cli/commands/get_memory_store.py", "description": "Memory store retrieval and access"},
        {"file_path": "ai/cli/commands/costdashboard.py", "description": "Cost dashboard with analytics and reporting"},
        {"file_path": "ai/cli/commands/write.py", "description": "Write operations with validation and logging"},
        {"file_path": "ai/cli/commands/estimate_tokens.py", "description": "Token estimation for cost prediction"},
        {"file_path": "ai/cli/commands/reposcanner.py", "description": "Repository scanning and analysis tools"},
        {"file_path": "ai/cli/commands/record_execution_metrics.py", "description": "Execution metrics recording and tracking"},
        {"file_path": "ai/cli/commands/get_usage_summary.py", "description": "Usage summary generation and reporting"},
        {"file_path": "ai/cli/commands/track_completion.py", "description": "Completion tracking and progress monitoring"},
        {"file_path": "ai/cli/commands/get_engine_metrics.py", "description": "Engine metrics collection and analysis"},
        {"file_path": "ai/cli/commands/track_read.py", "description": "Read operation tracking and analytics"},
        {"file_path": "ai/cli/commands/get_memory_analytics.py", "description": "Memory analytics and usage patterns"},
        {"file_path": "ai/cli/commands/magiccommand.py", "description": "Magic command processor with auto-completion"},
        {"file_path": "ai/cli/commands/create_agent_panel.py", "description": "Agent panel creation with UI components"},
        {"file_path": "ai/cli/commands/loopnode.py", "description": "Loop node management for workflow execution"},
        {"file_path": "ai/cli/commands/quick_cost_summary.py", "description": "Quick cost summary with real-time updates"},
        {"file_path": "ai/cli/commands/analyze_all_files.py", "description": "Comprehensive file analysis and reporting"}
    ]
    
    # Create orchestrator with 20 parallel workers for GPT-4o-mini test
    orchestrator = ParallelAutonomousOrchestrator(max_workers=20, budget_limit=5.0)
    
    # Run parallel implementation
    report = await orchestrator.run_parallel_batch(test_features)
    
    # Success evaluation
    if report["orchestration_summary"]["success_rate"] >= 0.8:
        print(f"\nðŸŽ‰ SUCCESS! MotherAgent parallel orchestration working perfectly!")
        print(f"âœ¨ Ready to scale to full 587-feature autonomous implementation!")
        speedup = 15 * 60 / report["orchestration_summary"]["duration"]  # Compare to 15min sequential
        print(f"ðŸš€ Speedup: {speedup:.1f}x faster than sequential processing!")
    else:
        print(f"\nâš ï¸  Some issues encountered. Review and adjust before full-scale.")
    
    return report["orchestration_summary"]["success_rate"] >= 0.5

if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nðŸ›‘ MotherAgent orchestration stopped by user")
        sys.exit(130)
